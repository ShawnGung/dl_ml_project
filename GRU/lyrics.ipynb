{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相邻采样生成训练样本 \n",
    "\n",
    "*\n",
    "输入的X:\n",
    "[[ 0.  1.  2.  3.  4.  5.],[18. 19. 20. 21. 22. 23.]]  \n",
    "对应的Y(错开):\n",
    "[[ 1.  2.  3.  4.  5.  6.],[19. 20. 21. 22. 23. 24.]]\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps):\n",
    "    \"\"\"Sample mini-batches in a consecutive order from sequential data.\"\"\"\n",
    "    corpus_indices = np.array(corpus_indices)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "    indices = corpus_indices[0 : batch_size * batch_len].reshape((\n",
    "        batch_size, batch_len))\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        X = indices[:, i : i + num_steps]\n",
    "        Y = indices[:, i + 1 : i + num_steps + 1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本处理类\n",
    "- 文本预处理,包括减少频率少的词,过滤标点符号等\n",
    "- 生成词到index 以及 index到词的字典\n",
    "- 两个方法来对句子进行 text到index的arr 或者 index的arr到text的互转"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "class TextConverter(object):\n",
    "    def __init__(self, max_vocab=5000):\n",
    "        \"\"\"建立一个字符索引转换器\n",
    "        Args:\n",
    "            text_path: 文本位置\n",
    "            max_vocab: 最大的单词数量\n",
    "        \"\"\"\n",
    "        \n",
    "        with zipfile.ZipFile('lyrics/jaychou_lyrics.txt.zip') as zin:\n",
    "            with zin.open('jaychou_lyrics.txt') as f:\n",
    "                text = f.read().decode('utf-8')\n",
    "        text = text.replace('\\n', ' ').replace('\\r', ' ').replace('，', ' ').replace('。', ' ')\n",
    "        text = self.preprocessing(text,max_vocab)\n",
    "\n",
    "        \n",
    "    def preprocessing(self,text,max_vocab):\n",
    "        #tonization\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        \n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        \n",
    "        text = ' '.join(words)\n",
    "        \n",
    "        # 计算单词出现频率并排序\n",
    "        vocab_count_list = Counter(text).most_common(max_vocab)\n",
    "        \n",
    "        # 如果超过最大值，截取频率最低的字符\n",
    "        vocab = [x[0] for x in vocab_count_list]\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.word_to_int_table = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.int_to_word_table = dict(enumerate(self.vocab))\n",
    "        self.corpus_indices = self.text_to_arr(text)\n",
    "        \n",
    "        return text\n",
    "        \n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def word_to_int(self, word):\n",
    "        if word in self.word_to_int_table:\n",
    "            return self.word_to_int_table[word]\n",
    "        else:\n",
    "            raise Exception('Unknown word!')\n",
    "\n",
    "    def int_to_word(self, index):\n",
    "        if index == len(self.vocab):\n",
    "            return '<unk>'\n",
    "        elif index < len(self.vocab):\n",
    "            return self.int_to_word_table[index]\n",
    "        else:\n",
    "            raise Exception('Unknown index!')\n",
    "\n",
    "    def text_to_arr(self, text):\n",
    "        arr = []\n",
    "        for word in text:\n",
    "            arr.append(self.word_to_int(word))\n",
    "        return np.array(arr)\n",
    "\n",
    "    def arr_to_text(self, arr):\n",
    "        words = []\n",
    "        for index in arr:\n",
    "            words.append(self.int_to_word(index))\n",
    "        return \"\".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert = TextConverter(max_vocab=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 512\n",
    "n_embedding = 512\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "vocab_size = convert.vocab_size()\n",
    "num_steps = 20\n",
    "top_n = 2 # 选取top几的词生成, 越大就随机性越大, 样本的多样性高\n",
    "predict_len = 15 #生成多长的歌词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面代码的问题：\n",
    "我们输入的转化词向量之后是\n",
    "X : [batch_size, n_step, vocab_size]\n",
    "\n",
    "我们transpose了X\n",
    "X : [n_step,batch_size,vocab_size]\n",
    "这是代表，我把每个batch_size的第一行单独拧出来凑一起。\n",
    "所以现在的情况是\n",
    "\n",
    "假如batch_size是2 , n_step 是 5 ， 忽略最里面vocab_size\n",
    "[\n",
    "[[1,2,3,4,5]],\n",
    "[[10,11,12,13,14]]\n",
    "]\n",
    "\n",
    "变成了\n",
    "\n",
    "[\n",
    "[1,10],[2,11],[3,12],[4,13],[5,14]\n",
    "]\n",
    "\n",
    "rnn 之后呢\n",
    "outputs [n_step, batch_size, n_hidden]\n",
    "\n",
    "经过view(-1,shape[2])\n",
    "变成了\n",
    "[\n",
    "[1,10,2,11,3,12,4,13,5,14]\n",
    "]\n",
    "再经过dense层\n",
    "[\n",
    "[1,10,2,11,3,12,4,13,5,14]\n",
    "]\n",
    "\n",
    "这明显有问题\n",
    "\n",
    "我们需要的是\n",
    "[1,2,3,4,5,10,11,12,13,14]这样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextRNN, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, n_hidden)\n",
    "#         self.rnn = nn.RNN(input_size=vocab_size, hidden_size=n_hidden)\n",
    "        self.rnn = nn.GRU(n_hidden, n_hidden, num_layers = 1)\n",
    "        self.dense = nn.Linear(n_hidden, vocab_size)\n",
    "\n",
    "    def forward(self, X, hidden):\n",
    "        # embedding层的输入不需要one - hot\n",
    "        X = self.embed(X)# X : [batch_size,n_step] -> # X : [batch_size,n_step, vocab_size]\n",
    "        X = X.transpose(0, 1) # X : [n_step, batch_size, vocab_size]\n",
    "        outputs, hidden = self.rnn(X, hidden)\n",
    "        # outputs : [n_step, batch_size, num_directions(=1) * n_hidden]\n",
    "        # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        le, mb, hd = outputs.shape\n",
    "        outputs = outputs.view(le * mb, hd)\n",
    "        outputs = self.dense(outputs) # [n_step * batch_size, num_classes]\n",
    "        # outputs : each output of each time step\n",
    "        # hidden : the hidden layer output at the last time step\n",
    "        outputs = outputs.view(le, mb, -1)\n",
    "        outputs = outputs.permute(1, 0, 2).contiguous()  # (batch, len, hidden)\n",
    "        return outputs.view(-1, outputs.shape[2]), hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的代码为什么就可以呢\n",
    "\n",
    "假如batch_size是2 , n_step 是 5 ， 忽略最里面vocab_size\n",
    "[\n",
    "[[1,2,3,4,5]],\n",
    "[[10,11,12,13,14]]\n",
    "]\n",
    "\n",
    "\n",
    "转置后变成了\n",
    "\n",
    "[\n",
    "[1,10],[2,11],[3,12],[4,13],[5,14]\n",
    "]\n",
    "\n",
    "rnn后变成 (5,2,n_hidden)\n",
    "\n",
    "view(10,n_hidden) 一下变成\n",
    "[\n",
    "[1,10,2,11,3,12,4,13,5,14]\n",
    "]\n",
    "\n",
    "dense后(10,vocab_size)\n",
    "[\n",
    "[1,10,2,11,3,12,4,13,5,14]\n",
    "]\n",
    "\n",
    "先回去原来的view(5,2,vocab_size)\n",
    "[\n",
    "[1,10],[2,11],[3,12],[4,13],[5,14]\n",
    "]\n",
    "在转置0,1 变成 (2,5,vocab_size)\n",
    "[[1,2,3,4,5],[10,11,12,13,14]]\n",
    "再continguous，重新分配内存\n",
    "然后view成一列好进行loss的计算\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, num_classes, embed_dim, hidden_size, \n",
    "                 num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.word_to_vec = nn.Embedding(num_classes, embed_dim)\n",
    "        self.rnn = nn.RNN(input_size=embed_dim, hidden_size=embed_dim)\n",
    "        self.project = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, hs=None):\n",
    "        batch = x.shape[0]\n",
    "        if hs is None:\n",
    "            hs = Variable(\n",
    "                torch.zeros(self.num_layers, batch, self.hidden_size))\n",
    "            if use_gpu:\n",
    "                hs = hs.cuda()\n",
    "        word_embed = self.word_to_vec(x)  # (batch, len, embed)\n",
    "        word_embed = word_embed.permute(1, 0, 2)  # (len, batch, embed)\n",
    "        out, h0 = self.rnn(word_embed, hs)  # (len, batch, hidden)\n",
    "        le, mb, hd = out.shape\n",
    "        out = out.view(le * mb, hd)\n",
    "        out = self.project(out)\n",
    "        out = out.view(le, mb, -1)\n",
    "        out = out.permute(1, 0, 2).contiguous()  # (batch, len, hidden)\n",
    "        return out.view(-1, out.shape[2]), h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = data_iter_consecutive(\n",
    "            convert.corpus_indices, batch_size = batch_size, num_steps = num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick_top_n是用来根据概率来采样生成的词\n",
    "*如果top_n = 1, 默认生成概率最高的词, 这样的多样性不高*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, top_n=1):\n",
    "    top_pred_prob, top_pred_label = torch.topk(preds, top_n, 1)\n",
    "    top_pred_prob /= torch.sum(top_pred_prob)\n",
    "    top_pred_prob = top_pred_prob.squeeze(0).detach().numpy()\n",
    "    top_pred_label = top_pred_label.squeeze(0).detach().numpy()\n",
    "    c = np.random.choice(top_pred_label, size=1, p=top_pred_prob)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测生成句子函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, num_chars, model, vocab_size , idx_to_char,\n",
    "                      char_to_idx, top_n = 2):\n",
    "    # 初始化隐藏状态\n",
    "    # hidden : [num_layers * num_directions, batch, hidden_size]\n",
    "    hidden = torch.zeros(1, 1, n_hidden)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        #取output中最后一个词作为输入,求下一词的输入 + hidden output\n",
    "        X = torch.LongTensor([[output[-1]]])\n",
    "        (Y, hidden) = model(X, hidden)  # 前向计算不需要传入模型参数\n",
    "        if t < len(prefix) - 1: #刚开始的提示词按照提示词加入\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            bestY = pick_top_n(Y, top_n = top_n)\n",
    "            output.append(int(bestY))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开嘆朗确擀紧脾P卑仓部缭龟死正酸'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TextRNN()\n",
    "# model = CharRNN(vocab_size, 512, 512, 1, 0.5)\n",
    "\n",
    "predict_rnn('分开', predict_len, model, convert.vocab_size , convert.int_to_word_table, convert.word_to_int_table,top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====epoch:1=====\n",
      "cost 5.701179, perplexity 440.96, time 11.78 sec\n",
      " - 珍惜 你的爱 你说你说你 你的手了\n",
      " - 天青色 我的爱 我不要我的 我不能 \n",
      " - 两个人 我不要 你的手了 我的爱你的\n",
      "====epoch:2=====\n",
      "cost 4.838658, perplexity 142.67, time 11.58 sec\n",
      " - 珍惜 你的爱 我的手 你的爱情 我\n",
      " - 天青色的 你的手 你的手中的 你说你\n",
      " - 两个人的手 我的爱情绪的 我的爱 你\n",
      "====epoch:3=====\n",
      "cost 4.151860, perplexity 69.02, time 11.65 sec\n",
      " - 珍惜 我们的脸上的 你说你的爱情 \n",
      " - 天青色的 你的手 你说你的手 你的手\n",
      " - 两个人的 我不要我的爱情绪 你说你的\n",
      "====epoch:4=====\n",
      "cost 3.624521, perplexity 39.02, time 11.57 sec\n",
      " - 珍惜 我不能再来 我的爱 你说不出\n",
      " - 天青色的 爱情悬崖的 我不要再见 我\n",
      " - 两个人 都是不会有 你说不了 我不能\n",
      "====epoch:5=====\n",
      "cost 3.161995, perplexity 24.32, time 11.63 sec\n",
      " - 珍惜 一统江山 这样的我 我不能够\n",
      " - 天青色的 时光 我想揍你的手 你说你\n",
      " - 两个人的 幸福呢 我不想再想 你的崩\n",
      "====epoch:6=====\n",
      "cost 2.741878, perplexity 16.16, time 11.45 sec\n",
      " - 珍惜一个 那么会有人 我的世界将你\n",
      " - 天青色 我们乘着阳 我不能再来 我想\n",
      " - 两个人的特 我的爱情悬在 我的爱 你\n",
      "====epoch:7=====\n",
      "cost 2.378531, perplexity 11.25, time 11.47 sec\n",
      " - 珍惜 我想揍你已经很难回头 我不想\n",
      " - 天青色 我的爱 我不能再来一个 我不\n",
      " - 两个人 都不公平也不能 不要再想你 \n",
      "====epoch:8=====\n",
      "cost 2.048591, perplexity 8.18, time 11.62 sec\n",
      " - 珍惜 我不能 别怪的 幸福中发芽 \n",
      " - 天青色等待 娘事都自然不关 这些 於\n",
      " - 两个人的特 观念不及逃 你说我不能再\n",
      "====epoch:9=====\n",
      "cost 1.766066, perplexity 6.20, time 11.45 sec\n",
      " - 珍惜一场悲剧 你的崩在我面前 深呼\n",
      " - 天青色等烟 我不想 再来一个秋天 看\n",
      " - 两个人 都是不是我不对 你说你不用麻\n",
      "====epoch:10=====\n",
      "cost 1.534451, perplexity 4.89, time 11.71 sec\n",
      " - 珍惜 这些爱只是我不能再想 你说你\n",
      " - 天青色等烟雨 而你我的世道上一行忧伤\n",
      " - 两个人的 幸福中发芽 如果我遇见你是\n",
      "====epoch:11=====\n",
      "cost 1.328499, perplexity 3.99, time 11.68 sec\n",
      " - 珍惜一切 就是那么会扯多你就撑跃过\n",
      " - 天青色 等待英雄也累不了我 就是开不\n",
      " - 两个人都听 那些 太琐过 你说你好累\n",
      "====epoch:12=====\n",
      "cost 1.176178, perplexity 3.34, time 11.73 sec\n",
      " - 珍惜 这样也没道理 只能永远赢不对\n",
      " - 天青色 等雨一起 等雨变强之中我 泪\n",
      " - 两个人都听到我的世道上一个人演戏 一\n",
      "====epoch:13=====\n",
      "cost 1.023757, perplexity 2.89, time 11.66 sec\n",
      " - 珍惜一场意外 你的崩溃 在窗外进行\n",
      " - 天青色等烟 我不想再想你说 不用问 \n",
      " - 两个人的特观 不要哭让你离开 距离别\n",
      "====epoch:14=====\n",
      "cost 0.889690, perplexity 2.54, time 11.70 sec\n",
      " - 珍惜一场 我会不会稍微微笑 快疯掉\n",
      " - 天青色 等到风吹不灭 繁在我怀念 等\n",
      " - 两个人都听 我用第一名字培 就是那条\n",
      "====epoch:15=====\n",
      "cost 0.780857, perplexity 2.29, time 11.41 sec\n",
      " - 珍惜一点 从他的手应该看着我 别怪\n",
      " - 天青色等烟雨 雨纷纷 旧故里的乾淨像\n",
      " - 两个人的回味你给过我纵容 脸上汹涌失\n",
      "====epoch:16=====\n",
      "cost 0.699285, perplexity 2.09, time 11.52 sec\n",
      " - 珍惜 这样也好不好 你的笑容 已无\n",
      " - 天青色等烟雨天 是我不懂得太快 我会\n",
      " - 两个人都有我 好暗 我们同个字歌 妈\n",
      "====epoch:17=====\n",
      "cost 0.634508, perplexity 1.94, time 11.50 sec\n",
      " - 珍惜 这生命就这是做错 也只能永远\n",
      " - 天青色等烟雨 雨纷纷 雨还没停在 你\n",
      " - 两个人的回味 我向前去追有你的微笑像\n",
      "====epoch:18=====\n",
      "cost 0.580592, perplexity 1.82, time 11.62 sec\n",
      " - 珍惜 这些我都做得更多 想这样没有\n",
      " - 天青色等待 以牙还牙 在风中持续着甜\n",
      " - 两个人的特 观念不能太薄 如你默认 \n",
      "====epoch:19=====\n",
      "cost 0.498898, perplexity 1.72, time 11.45 sec\n",
      " - 珍惜没有 我有一点稚气 想赢不了 \n",
      " - 天青色等烟 雨在墙壁上的皇朝 飘落后\n",
      " - 两个人的回旋踢 周杰伦 一本不想太多\n",
      "====epoch:20=====\n",
      "cost 0.479577, perplexity 1.65, time 11.59 sec\n",
      " - 珍惜 这些弓箭手机传中文字 那传输\n",
      " - 天青色想回 你说你好久不见罪的国度 \n",
      " - 两个人的特 你会前度 能不能给我抬头\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    l_sum, n, start = 0.0, 0, time.time()\n",
    "    data_iter = data_iter_consecutive(\n",
    "            convert.corpus_indices, batch_size = batch_size, num_steps = num_steps)\n",
    "    print('====epoch:'+ str(epoch+1) +\"=====\")\n",
    "    for X, Y in data_iter:\n",
    "        X = torch.tensor(X)\n",
    "        Y = torch.tensor(Y)\n",
    "        hidden = torch.randn(1, batch_size, n_hidden)\n",
    "        (output, hidden) = model(X, hidden)\n",
    "        loss = criterion(output, Y.view(-1))    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()   \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        l_sum += loss.item() * Y.shape[0]\n",
    "        n += Y.shape[0]\n",
    "    print('cost %.6f, perplexity %.2f, time %.2f sec' % (\n",
    "                loss, math.exp(l_sum / n), time.time() - start))\n",
    "    print(' -', predict_rnn(\n",
    "        '珍惜', predict_len, model, convert.vocab_size, convert.int_to_word_table, convert.word_to_int_table,top_n))\n",
    "    print(' -', predict_rnn(\n",
    "        '天青色', predict_len, model, convert.vocab_size, convert.int_to_word_table, convert.word_to_int_table,top_n))\n",
    "    print(' -', predict_rnn(\n",
    "        '两个人', predict_len, model, convert.vocab_size, convert.int_to_word_table, convert.word_to_int_table,top_n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

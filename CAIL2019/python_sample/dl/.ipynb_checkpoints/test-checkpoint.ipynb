{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import jieba\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from config import *\n",
    "\n",
    "\n",
    "total_nlp_features = 16\n",
    "\n",
    "\n",
    "#对短信中的用户名前缀和内部的url链接进行过滤删除\n",
    "def filter(line):\n",
    "    #前缀的正则\n",
    "    username_regex = re.compile(r\"^\\d+::\")\n",
    "    #URL，为了防止对中文的过滤，所以使用[a-zA-Z0-9]而不是\\w\n",
    "    url_regex = re.compile(r\"\"\"\n",
    "        (https?://)?\n",
    "        ([a-zA-Z0-9]+)\n",
    "        (\\.[a-zA-Z0-9]+)\n",
    "        (\\.[a-zA-Z0-9]+)*\n",
    "        (/[a-zA-Z0-9]+)*\n",
    "    \"\"\", re.VERBOSE|re.IGNORECASE)\n",
    "    #剔除日期\n",
    "    data_regex = re.compile(u\"\"\"        #utf-8编码\n",
    "        年 |\n",
    "        月 |\n",
    "        日 |\n",
    "        (周一) |\n",
    "        (周二) | \n",
    "        (周三) | \n",
    "        (周四) | \n",
    "        (周五) | \n",
    "        (周六)\n",
    "    \"\"\", re.VERBOSE)\n",
    "    #剔除所有数字\n",
    "    decimal_regex = re.compile(r\"[^a-zA-Z]\\d+\")\n",
    "    #剔除空格\n",
    "    space_regex = re.compile(r\"\\s+\")\n",
    "\n",
    "    line = username_regex.sub(r\"\", line)\n",
    "    line = url_regex.sub(r\"\", line)\n",
    "    line = data_regex.sub(r\"\", line)\n",
    "    line = decimal_regex.sub(r\"\", line)\n",
    "    line = space_regex.sub(r\"\", line)\n",
    "\n",
    "    return line\n",
    "\n",
    "def stopwordslist(filepath):  \n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "def trans(sentence,cutwordslist = None):\n",
    "    sentence = filter(sentence)\n",
    "    sentence_seged = jieba.cut(sentence,cut_all=False)\n",
    "    stopwords = stopwordslist('data/stopwords.txt')  # 这里加载停用词的路径      \n",
    "    outstr = ''  \n",
    "    for word in sentence_seged:\n",
    "        if word not in stopwords:  \n",
    "            if word != '\\t':\n",
    "                if cutwordslist!= None:\n",
    "                    cutwordslist += [word]\n",
    "                outstr += word  \n",
    "                outstr += \" \"\n",
    "    return outstr\n",
    "\n",
    "X = []\n",
    "f = open(\"../input/input.txt\", \"r\", encoding=\"utf8\")\n",
    "for line in f:\n",
    "    x = json.loads(line)\n",
    "    X.append([x[\"A\"],x[\"B\"],x[\"C\"]])\n",
    "    \n",
    "X = np.array(X)\n",
    "indices = np.arange(X.shape[0])\n",
    "X_train, X_valid,indices_train,indices_valid = train_test_split(X,indices, test_size = 0.1, random_state = 1996)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = list()\n",
    "[rows, cols] = X_train.shape\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        se.append(X_train[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/8j/p8231m8944s1l43bpl8h5t780000gn/T/jieba.cache\n",
      "Loading model cost 0.678 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "cutwordslist = []\n",
    "data = se\n",
    "for a in range(0, len(data)):\n",
    "    data[a] = trans(data[a],cutwordslist)\n",
    "\n",
    "# outputwords = dict(Counter(cutwordslist))\n",
    "# outputwords_sorted = sorted(outputwords.items(), key= lambda x : x[1], reverse=True)[:100]\n",
    "# print(outputwords_sorted)\n",
    "\n",
    "tfidf_model = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\").fit(data)\n",
    "sparse_result = tfidf_model.transform(data)\n",
    "\n",
    "\n",
    "[rows, cols] = X_valid.shape\n",
    "count = 0\n",
    "\n",
    "valid_tfidf_1 = []\n",
    "valid_tfidf_2 = []\n",
    "for i in range(rows):\n",
    "    d1 = X_valid[i][0]\n",
    "    d2 = X_valid[i][1]\n",
    "    d3 = X_valid[i][2]\n",
    "    \n",
    "    y = [\n",
    "        trans(d1),\n",
    "        trans(d2),\n",
    "        trans(d3)\n",
    "    ]\n",
    "\n",
    "    y = tfidf_model.transform(y)\n",
    "    y = y.todense()\n",
    "    \n",
    "    v1 = np.sum(np.dot(y[0], np.transpose(y[1])))\n",
    "    v2 = np.sum(np.dot(y[0], np.transpose(y[2])))\n",
    "    valid_tfidf_1.append(v1)\n",
    "    valid_tfidf_2.append(v2)\n",
    "    \n",
    "        \n",
    "valid_tfidf_1 = np.array(valid_tfidf_1).reshape(-1,1)\n",
    "valid_tfidf_2 = np.array(valid_tfidf_2).reshape(-1,1)\n",
    "\n",
    "[rows, cols] = X_train.shape\n",
    "\n",
    "train_tfidf_1 = []\n",
    "train_tfidf_2 = []\n",
    "for i in range(rows):\n",
    "    d1 = X_train[i][0]\n",
    "    d2 = X_train[i][1]\n",
    "    d3 = X_train[i][2]\n",
    "    \n",
    "    y = [\n",
    "        trans(d1),\n",
    "        trans(d2),\n",
    "        trans(d3)\n",
    "    ]\n",
    "\n",
    "    y = tfidf_model.transform(y)\n",
    "    y = y.todense()\n",
    "    \n",
    "    v1 = np.sum(np.dot(y[0], np.transpose(y[1])))\n",
    "    v2 = np.sum(np.dot(y[0], np.transpose(y[2])))\n",
    "    train_tfidf_1.append(v1)\n",
    "    train_tfidf_2.append(v2)\n",
    "    \n",
    "train_tfidf_1 = np.array(train_tfidf_1).reshape(-1,1)\n",
    "train_tfidf_2 = np.array(train_tfidf_2).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 1)\n",
      "(450, 1)\n",
      "(50, 1)\n",
      "(50, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_tfidf_1.shape)\n",
    "print(train_tfidf_2.shape)\n",
    "print(valid_tfidf_1.shape)\n",
    "print(valid_tfidf_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nlp features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 16)\n",
      "(450, 16)\n",
      "(50, 16)\n",
      "(50, 16)\n"
     ]
    }
   ],
   "source": [
    "nlp_feature_train_1 = pd.read_csv(FEATURE_TRAIN_1).values\n",
    "nlp_feature_train_2 = pd.read_csv(FEATURE_TRAIN_2).values\n",
    "\n",
    "train_1 = nlp_feature_train_1[indices_train]\n",
    "train_2 = nlp_feature_train_2[indices_train]\n",
    "valid_1 = nlp_feature_train_1[indices_valid]\n",
    "valid_2 = nlp_feature_train_2[indices_valid]\n",
    "\n",
    "\n",
    "print(train_1.shape)\n",
    "print(train_2.shape)\n",
    "print(valid_1.shape)\n",
    "print(valid_2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_1 = np.concatenate((train_1,train_tfidf_1),axis = 1)\n",
    "new_train_2 = np.concatenate((train_2,train_tfidf_2),axis = 1)\n",
    "new_valid_1 = np.concatenate((valid_1,valid_tfidf_1),axis = 1)\n",
    "new_valid_2 = np.concatenate((valid_2,valid_tfidf_2),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 17)\n",
      "(450, 17)\n",
      "(50, 17)\n",
      "(50, 17)\n"
     ]
    }
   ],
   "source": [
    "print(new_train_1.shape)\n",
    "print(new_train_2.shape)\n",
    "print(new_valid_1.shape)\n",
    "print(new_valid_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((new_train_1, new_train_2,new_valid_1,new_valid_2)))\n",
    "new_train_1 = ss.transform(new_train_1)\n",
    "new_train_2 = ss.transform(new_train_2)\n",
    "new_valid_1 = ss.transform(new_valid_1)\n",
    "new_valid_2 = ss.transform(new_valid_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 17)\n",
      "(450, 17)\n",
      "(50, 17)\n",
      "(50, 17)\n"
     ]
    }
   ],
   "source": [
    "print(new_train_1.shape)\n",
    "print(new_train_2.shape)\n",
    "print(new_valid_1.shape)\n",
    "print(new_valid_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [i for i in range(450)]\n",
    "shuffle(index)\n",
    "new_train_1 = new_train_1[index]\n",
    "new_train_2 = new_train_2[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [i for i in range(50)]\n",
    "shuffle(index)\n",
    "new_valid_1 = new_valid_1[index]\n",
    "new_valid_2 = new_valid_2[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.autograd import Variable \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义一个构建神经网络的类 \n",
    "class Net(torch.nn.Module): # 继承torch.nn.Module类 \n",
    "    def __init__(self, n_feature = 17, n_hidden = 10, n_output = 5): \n",
    "        super(Net, self).__init__() # 获得Net类的超类（父类）的构造方法 \n",
    "        # 定义神经网络的每层结构形式 \n",
    "        # 各个层的信息都是Net类对象的属性 \n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden) # 隐藏层线性输出 \n",
    "        \n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output) # 输出层线性输出 \n",
    "        self.margin = 0.05\n",
    "        \n",
    "        \n",
    "    def doc_encoding(self, d):\n",
    "        d = F.relu(self.hidden(d)) # 对隐藏层的输出进行relu激活 \n",
    "        d = self.predict(d)  # batch_size x 5\n",
    "        return d1\n",
    "\n",
    "    # 将各层的神经元搭建成完整的神经网络的前向通路 \n",
    "    def forward(self,d1,d2,d3): \n",
    "        d1 = self.doc_encoding(d1)\n",
    "        \n",
    "        d2 = self.doc_encoding(d2)\n",
    "        \n",
    "        d3 = self.doc_encoding(d3)\n",
    "        \n",
    "        pos_sim=F.cosine_similarity(d1, d2)\n",
    "        neg_sm=F.cosine_similarity(d1, d3)\n",
    "        \n",
    "        loss=(self.margin-good_sim+bad_sim).clamp(min=1e-6).mean()\n",
    "        return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(17, 10, 5)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5) # 传入网络参数和学习率 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(300): \n",
    "    prediction = net(x) # 把数据x喂给net，输出预测值 \n",
    "    loss = loss_function(prediction, y) # 计算两者的误差，要注意两个参数的顺序 \n",
    "    optimizer.zero_grad() # 清空上一步的更新参数值 \n",
    "    loss.backward() # 误差反相传播，计算新的更新参数值 \n",
    "    optimizer.step() # 将计算得到的更新值赋给net.parameters() \n",
    "    print('loss:', loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/8j/p8231m8944s1l43bpl8h5t780000gn/T/jieba.cache\n",
      "Loading model cost 0.608 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import jieba\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from config import *\n",
    "import os\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def filter(line):\n",
    "    #剔除日期\n",
    "    data_regex = re.compile(u\"\"\"        #utf-8编码\n",
    "        年 |\n",
    "        月 |\n",
    "        日 |\n",
    "        (周一) |\n",
    "        (周二) | \n",
    "        (周三) | \n",
    "        (周四) | \n",
    "        (周五) | \n",
    "        (周六)\n",
    "    \"\"\", re.VERBOSE)\n",
    "    #剔除所有数字\n",
    "    decimal_regex = re.compile(r\"[^a-zA-Z]\\d+\")\n",
    "\n",
    "    line = data_regex.sub(r\"\", line)\n",
    "    line = decimal_regex.sub(r\"\", line)\n",
    "\n",
    "    return line\n",
    "\n",
    "def trans(sentence,cutwordslist = None):\n",
    "    texts_cut = [word for word in jieba.lcut(filter(sentence)) if len(word) > 1]\n",
    "    outstr = ' '.join(texts_cut)\n",
    "    return outstr\n",
    "\n",
    "\n",
    "se = set()\n",
    "f = open(\"../input/input.txt\", \"r\", encoding=\"utf8\")\n",
    "for line in f:\n",
    "    x = json.loads(line)\n",
    "    se.add(x[\"A\"])\n",
    "    se.add(x[\"B\"])\n",
    "    se.add(x[\"C\"])\n",
    "    \n",
    "cutwordslist = []\n",
    "data = list(se)\n",
    "for a in range(0, len(data)):\n",
    "    data[a] = trans(data[a],cutwordslist)\n",
    "\n",
    "# outputwords = dict(Counter(cutwordslist))\n",
    "# outputwords_sorted = sorted(outputwords.items(), key= lambda x : x[1], reverse=True)[:100]\n",
    "\n",
    "# print(outputwords_sorted)\n",
    "\n",
    "tfidf_model = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\").fit(data)\n",
    "sparse_result = tfidf_model.transform(data)\n",
    "\n",
    "f = open(\"../input/input.txt\", \"r\", encoding=\"utf8\")\n",
    "ouf = open(\"../output/output.txt\", \"w\", encoding=\"utf8\")\n",
    "model = Doc2Vec.load(os.path.join(MODEL_PATH,DOC2VEC))  # you can continue training with the loaded model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_doc = []\n",
    "# for line in f:\n",
    "#     x = json.loads(line)\n",
    "    \n",
    "#     y = [\n",
    "#         trans(x[\"A\"]),\n",
    "#         trans(x[\"B\"]),\n",
    "#         trans(x[\"C\"])\n",
    "#     ]\n",
    "\n",
    "#     y = tfidf_model.transform(y)\n",
    "#     y = y.todense()\n",
    "#     all_doc.append(y[0].reshape(-1))\n",
    "#     all_doc.append(y[1].reshape(-1))\n",
    "#     all_doc.append(y[2].reshape(-1))\n",
    "    \n",
    "# all_doc = np.array(all_doc).reshape(1500,-1)\n",
    "# print(all_doc.shape)\n",
    "\n",
    "# # \"\"\"特征降维：lsa\"\"\"\n",
    "# # print(\"lsa......\")\n",
    "# # lsa = TruncatedSVD(n_components=100)#TruncatedSVD是SVD的变形，只计算用户指定的最大的K，个奇异值。\n",
    "# # all_doc = lsa.fit_transform(all_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../input/input.txt\", \"r\", encoding=\"utf8\")\n",
    "\n",
    "\n",
    "X_train = []\n",
    "for line in f:\n",
    "    x = json.loads(line)\n",
    "    y = [\n",
    "        model.infer_vector(trans(x[\"A\"]).split()),\n",
    "        model.infer_vector(trans(x[\"B\"]).split()),\n",
    "        model.infer_vector(trans(x[\"C\"]).split())\n",
    "    ]\n",
    "    \n",
    "    X_train.append(y)\n",
    "X_train = np.array(X_train)\n",
    "X_train, X_valid = train_test_split(X_train, test_size = 0.1, random_state = 1996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.autograd import Variable \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义一个构建神经网络的类 \n",
    "class Net(torch.nn.Module): # 继承torch.nn.Module类 \n",
    "    def __init__(self, n_feature = 4492, n_hidden = 256, n_output = 300): \n",
    "        super(Net, self).__init__() # 获得Net类的超类（父类）的构造方法 \n",
    "        # 定义神经网络的每层结构形式 \n",
    "        # 各个层的信息都是Net类对象的属性 \n",
    "        self.hidden1 = torch.nn.Linear(n_feature, 4*n_hidden) # 隐藏层线性输出 \n",
    "        #self.bn1 = torch.nn.BatchNorm1d(num_features=4*n_hidden)\n",
    "        self.dropout1 = torch.nn.Dropout(0.8)\n",
    "        \n",
    "        self.hidden2 = torch.nn.Linear(4*n_hidden, 4*n_hidden) # 隐藏层线性输出 \n",
    "        #self.bn2 = torch.nn.BatchNorm1d(num_features=2*n_hidden)\n",
    "        self.dropout2 = torch.nn.Dropout(0.5)\n",
    "        self.predict = torch.nn.Linear(4*n_hidden, n_output) # 输出层线性输出 \n",
    "        self.margin = 0.5\n",
    "        \n",
    "        \n",
    "    def doc_encoding(self, d):\n",
    "        d = F.relu(self.hidden1(d))# 对隐藏层的输出进行relu激活 \n",
    "        d = self.dropout1(d)\n",
    "        d = F.relu(self.hidden2(d))\n",
    "        d = self.dropout2(d)\n",
    "        d = self.predict(d)  # batch_size x 5\n",
    "        return d\n",
    "\n",
    "    # 将各层的神经元搭建成完整的神经网络的前向通路 \n",
    "    def forward(self,d1,d2,d3): \n",
    "        d1 = self.doc_encoding(d1)\n",
    "        \n",
    "        d2 = self.doc_encoding(d2)\n",
    "        \n",
    "        d3 = self.doc_encoding(d3)\n",
    "        \n",
    "        pos_sim=F.cosine_similarity(d1, d2)\n",
    "        neg_sm=F.cosine_similarity(d1, d3)\n",
    "        \n",
    "        loss=(self.margin-pos_sim+neg_sm).clamp(min=1e-6).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lossAndAcc(net):\n",
    "    net.eval()\n",
    "    l_sum ,acc = 0.0 , 0.0\n",
    "    count = 0\n",
    "    n=0\n",
    "    for each in X_valid:\n",
    "        d1 = each[0]\n",
    "        d2 = each[1]\n",
    "        d3 = each[2]\n",
    "        \n",
    "        d1 = torch.tensor(d1).view(1,-1).float()\n",
    "        d2 = torch.tensor(d2).view(1,-1).float()\n",
    "        d3 = torch.tensor(d3).view(1,-1).float()\n",
    "\n",
    "        d1_encoding = net.doc_encoding(d1)\n",
    "        d2_encoding = net.doc_encoding(d2)\n",
    "        d3_encoding = net.doc_encoding(d3)\n",
    "\n",
    "\n",
    "        d1_2 = F.cosine_similarity(d1_encoding,d2_encoding)\n",
    "\n",
    "        d1_3 = F.cosine_similarity(d1_encoding,d3_encoding)\n",
    "\n",
    "        if d1_2 > d1_3:\n",
    "            count+=1\n",
    "        l_sum += net(d1,d2,d3)\n",
    "        n+=1\n",
    "    net.train()\n",
    "    return l_sum / n, count / X_valid.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(network, path = MODEL_PATH, name=MODEL_NAME):\n",
    "#     torch.save(network, os.path.join(path, name))\n",
    "    state = network.state_dict()\n",
    "    for key in state:\n",
    "        state[key] = state[key].clone().cpu()\n",
    "    torch.save(network.state_dict(),os.path.join(path,name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(X_train.shape[2],256,300)\n",
    "# 定义优化器和损失函数 \n",
    "optimizer = Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "best_loss = 100\n",
    "for t in range(100):\n",
    "    train_lsum, n , acc = 0.0, 0, 0.0\n",
    "    count = 0\n",
    "    for each in tqdm(X_train):\n",
    "        d1 = each[0]\n",
    "        d2 = each[1]\n",
    "        d3 = each[2]\n",
    "        d1 = torch.tensor(d1).view(1,-1).float()\n",
    "        d2 = torch.tensor(d2).view(1,-1).float()\n",
    "        d3 = torch.tensor(d3).view(1,-1).float()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        d1_encoding = net.doc_encoding(d1)\n",
    "        d2_encoding = net.doc_encoding(d2)\n",
    "        d3_encoding = net.doc_encoding(d3)\n",
    "\n",
    "\n",
    "        d1_2 = F.cosine_similarity(d1_encoding,d2_encoding)\n",
    "\n",
    "        d1_3 = F.cosine_similarity(d1_encoding,d3_encoding)\n",
    "\n",
    "        if d1_2 > d1_3:\n",
    "            count+=1\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = net(d1,d2,d3)\n",
    "        train_lsum += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        n+=1\n",
    "    loss,acc = evaluate_lossAndAcc(net)\n",
    "    print(\"epoch %d | train_loss : %.4f | train_acc : %.4f | valid loss : %.4f | valid_acc : %.4f\" \n",
    "                       % (t, train_lsum/n,count / X_train.shape[0],loss, acc))\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        print('save the current best model | loss : %.4f | valid acc : %.4f' % (best_loss.item(), acc))\n",
    "        save_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count= 0 \n",
    "net.eval()\n",
    "for each in tqdm(X_valid):\n",
    "    d1 = each[0]\n",
    "    d2 = each[1]\n",
    "    d3 = each[2]\n",
    "    \n",
    "    d1 = torch.tensor(d1).view(1,-1).float()\n",
    "    d2 = torch.tensor(d2).view(1,-1).float()\n",
    "    d3 = torch.tensor(d3).view(1,-1).float()\n",
    "\n",
    "\n",
    "    d1_encoding = net.doc_encoding(d1)\n",
    "    d2_encoding = net.doc_encoding(d2)\n",
    "    d3_encoding = net.doc_encoding(d3)\n",
    "\n",
    "\n",
    "    d1_2 = F.cosine_similarity(d1_encoding,d2_encoding)\n",
    "\n",
    "    d1_3 = F.cosine_similarity(d1_encoding,d3_encoding)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if d1_2 > d1_3:\n",
    "        count+=1\n",
    "print('Acc :', count/X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 552.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train : 500\n",
      "B acc : 0.084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_dict = torch.load(os.path.join(MODEL_PATH, MODEL_NAME))\n",
    "net = Net(X_train.shape[2],256,300)\n",
    "\n",
    "net.load_state_dict(model_dict)\n",
    "net.eval()\n",
    "\n",
    "\n",
    "X_all = np.vstack((X_train, X_valid))\n",
    "\n",
    "\n",
    "test_y = []\n",
    "\n",
    "count= 0 \n",
    "for each in tqdm(X_valid):\n",
    "    d1 = each[0]\n",
    "    d2 = each[1]\n",
    "    d3 = each[2]\n",
    "    \n",
    "    d1 = torch.tensor(d1).view(1,-1).float()\n",
    "    d2 = torch.tensor(d2).view(1,-1).float()\n",
    "    d3 = torch.tensor(d3).view(1,-1).float()\n",
    "\n",
    "\n",
    "    d1_encoding = net.doc_encoding(d1)\n",
    "    d2_encoding = net.doc_encoding(d2)\n",
    "    d3_encoding = net.doc_encoding(d3)\n",
    "\n",
    "    d1_2 = F.cosine_similarity(d1_encoding,d2_encoding)\n",
    "\n",
    "    d1_3 = F.cosine_similarity(d1_encoding,d3_encoding)\n",
    "    \n",
    "    if d1_2 > d1_3:\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "        \n",
    "print('X_train :', X_all.shape[0])\n",
    "print('B acc :', count/X_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/8j/p8231m8944s1l43bpl8h5t780000gn/T/jieba.cache\n",
      "Loading model cost 0.608 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import jieba\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from config import *\n",
    "import os\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def filter(line):\n",
    "    #剔除日期\n",
    "    data_regex = re.compile(u\"\"\"        #utf-8编码\n",
    "        年 |\n",
    "        月 |\n",
    "        日 |\n",
    "        (周一) |\n",
    "        (周二) | \n",
    "        (周三) | \n",
    "        (周四) | \n",
    "        (周五) | \n",
    "        (周六)\n",
    "    \"\"\", re.VERBOSE)\n",
    "    #剔除所有数字\n",
    "    decimal_regex = re.compile(r\"[^a-zA-Z]\\d+\")\n",
    "\n",
    "    line = data_regex.sub(r\"\", line)\n",
    "    line = decimal_regex.sub(r\"\", line)\n",
    "\n",
    "    return line\n",
    "\n",
    "def trans(sentence,cutwordslist = None):\n",
    "    texts_cut = [word for word in jieba.lcut(filter(sentence)) if len(word) > 1]\n",
    "    outstr = ' '.join(texts_cut)\n",
    "    return outstr\n",
    "\n",
    "\n",
    "se = set()\n",
    "f = open(\"../input/input.txt\", \"r\", encoding=\"utf8\")\n",
    "for line in f:\n",
    "    x = json.loads(line)\n",
    "    se.add(x[\"A\"])\n",
    "    se.add(x[\"B\"])\n",
    "    se.add(x[\"C\"])\n",
    "    \n",
    "cutwordslist = []\n",
    "data = list(se)\n",
    "for a in range(0, len(data)):\n",
    "    data[a] = trans(data[a],cutwordslist)\n",
    "\n",
    "# outputwords = dict(Counter(cutwordslist))\n",
    "# outputwords_sorted = sorted(outputwords.items(), key= lambda x : x[1], reverse=True)[:100]\n",
    "\n",
    "# print(outputwords_sorted)\n",
    "\n",
    "tfidf_model = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\").fit(data)\n",
    "sparse_result = tfidf_model.transform(data)\n",
    "\n",
    "f = open(\"../input/input.txt\", \"r\", encoding=\"utf8\")\n",
    "ouf = open(\"../output/output.txt\", \"w\", encoding=\"utf8\")\n",
    "model = Doc2Vec.load(os.path.join(MODEL_PATH,DOC2VEC))  # you can continue training with the loaded model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_doc = []\n",
    "# for line in f:\n",
    "#     x = json.loads(line)\n",
    "    \n",
    "#     y = [\n",
    "#         trans(x[\"A\"]),\n",
    "#         trans(x[\"B\"]),\n",
    "#         trans(x[\"C\"])\n",
    "#     ]\n",
    "\n",
    "#     y = tfidf_model.transform(y)\n",
    "#     y = y.todense()\n",
    "#     all_doc.append(y[0].reshape(-1))\n",
    "#     all_doc.append(y[1].reshape(-1))\n",
    "#     all_doc.append(y[2].reshape(-1))\n",
    "    \n",
    "# all_doc = np.array(all_doc).reshape(1500,-1)\n",
    "# print(all_doc.shape)\n",
    "\n",
    "# # \"\"\"特征降维：lsa\"\"\"\n",
    "# # print(\"lsa......\")\n",
    "# # lsa = TruncatedSVD(n_components=100)#TruncatedSVD是SVD的变形，只计算用户指定的最大的K，个奇异值。\n",
    "# # all_doc = lsa.fit_transform(all_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../input/input.txt\", \"r\", encoding=\"utf8\")\n",
    "\n",
    "\n",
    "X_train = []\n",
    "for line in f:\n",
    "    x = json.loads(line)\n",
    "    y = [\n",
    "        model.infer_vector(trans(x[\"A\"]).split()),\n",
    "        model.infer_vector(trans(x[\"B\"]).split()),\n",
    "        model.infer_vector(trans(x[\"C\"]).split())\n",
    "    ]\n",
    "    \n",
    "    X_train.append(y)\n",
    "X_train = np.array(X_train)\n",
    "X_train, X_valid = train_test_split(X_train, test_size = 0.1, random_state = 1996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 3, 300)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.autograd import Variable \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义一个构建神经网络的类 \n",
    "class Net(torch.nn.Module): # 继承torch.nn.Module类 \n",
    "    def __init__(self, n_feature = 4492, n_hidden = 256, n_output = 300): \n",
    "        super(Net, self).__init__() # 获得Net类的超类（父类）的构造方法 \n",
    "        # 定义神经网络的每层结构形式 \n",
    "        # 各个层的信息都是Net类对象的属性 \n",
    "        self.hidden1 = torch.nn.Linear(n_feature, 4*n_hidden) # 隐藏层线性输出 \n",
    "        #self.bn1 = torch.nn.BatchNorm1d(num_features=4*n_hidden)\n",
    "        self.dropout1 = torch.nn.Dropout(0.8)\n",
    "        \n",
    "        self.hidden2 = torch.nn.Linear(4*n_hidden, 4*n_hidden) # 隐藏层线性输出 \n",
    "        #self.bn2 = torch.nn.BatchNorm1d(num_features=2*n_hidden)\n",
    "        self.dropout2 = torch.nn.Dropout(0.5)\n",
    "        self.predict = torch.nn.Linear(4*n_hidden, n_output) # 输出层线性输出 \n",
    "        self.margin = 1\n",
    "        \n",
    "        \n",
    "    def doc_encoding(self, d):\n",
    "        d = F.relu(self.hidden1(d))# 对隐藏层的输出进行relu激活 \n",
    "        d = self.dropout1(d)\n",
    "        d = F.relu(self.hidden2(d))\n",
    "        d = self.dropout2(d)\n",
    "        d = self.predict(d)  # batch_size x 5\n",
    "        return d\n",
    "\n",
    "    # 将各层的神经元搭建成完整的神经网络的前向通路 \n",
    "    def forward(self,d1,d2,d3): \n",
    "        d1 = self.doc_encoding(d1)\n",
    "        \n",
    "        d2 = self.doc_encoding(d2)\n",
    "        \n",
    "        d3 = self.doc_encoding(d3)\n",
    "        \n",
    "        pos_sim=F.cosine_similarity(d1, d2)\n",
    "        neg_sm=F.cosine_similarity(d1, d3)\n",
    "        \n",
    "        loss=(self.margin-pos_sim+neg_sm).clamp(min=1e-6).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lossAndAcc(net):\n",
    "    net.eval()\n",
    "    l_sum ,acc = 0.0 , 0.0\n",
    "    count = 0\n",
    "    n=0\n",
    "    for each in X_valid:\n",
    "        d1 = each[0]\n",
    "        d2 = each[1]\n",
    "        d3 = each[2]\n",
    "        \n",
    "        d1 = torch.tensor(d1).view(1,-1).float()\n",
    "        d2 = torch.tensor(d2).view(1,-1).float()\n",
    "        d3 = torch.tensor(d3).view(1,-1).float()\n",
    "\n",
    "        d1_encoding = net.doc_encoding(d1)\n",
    "        d2_encoding = net.doc_encoding(d2)\n",
    "        d3_encoding = net.doc_encoding(d3)\n",
    "\n",
    "\n",
    "        d1_2 = F.cosine_similarity(d1_encoding,d2_encoding)\n",
    "\n",
    "        d1_3 = F.cosine_similarity(d1_encoding,d3_encoding)\n",
    "\n",
    "        if d1_2 > d1_3:\n",
    "            count+=1\n",
    "        l_sum += net(d1,d2,d3)\n",
    "        n+=1\n",
    "    net.train()\n",
    "    return l_sum / n, count / X_valid.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(network, path = MODEL_PATH, name=MODEL_NAME):\n",
    "#     torch.save(network, os.path.join(path, name))\n",
    "    state = network.state_dict()\n",
    "    for key in state:\n",
    "        state[key] = state[key].clone().cpu()\n",
    "    torch.save(network.state_dict(),os.path.join(path,name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:09<00:00, 49.51it/s]\n",
      "  1%|          | 5/450 [00:00<00:09, 48.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train_loss : 0.9831 | train_acc : 0.5644 | valid loss : 0.9267 | valid_acc : 0.7000\n",
      "save the current best model | loss : 0.9267 | valid acc : 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:09<00:00, 46.86it/s]\n",
      "  1%|          | 5/450 [00:00<00:09, 46.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 | train_loss : 0.9451 | train_acc : 0.5644 | valid loss : 0.9686 | valid_acc : 0.6600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:09<00:00, 45.87it/s]\n",
      "  1%|          | 5/450 [00:00<00:10, 42.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 | train_loss : 0.9309 | train_acc : 0.5867 | valid loss : 0.8258 | valid_acc : 0.5800\n",
      "save the current best model | loss : 0.8258 | valid acc : 0.5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:14<00:00, 32.01it/s]\n",
      "  1%|          | 4/450 [00:00<00:12, 34.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 | train_loss : 0.8589 | train_acc : 0.6556 | valid loss : 0.6545 | valid_acc : 0.7200\n",
      "save the current best model | loss : 0.6545 | valid acc : 0.7200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:12<00:00, 36.07it/s]\n",
      "  1%|          | 4/450 [00:00<00:11, 38.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 | train_loss : 0.8276 | train_acc : 0.6133 | valid loss : 0.7264 | valid_acc : 0.7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:11<00:00, 33.13it/s]\n",
      "  1%|          | 4/450 [00:00<00:11, 38.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 | train_loss : 0.7800 | train_acc : 0.6711 | valid loss : 0.6597 | valid_acc : 0.7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:11<00:00, 42.21it/s]\n",
      "  1%|          | 4/450 [00:00<00:11, 39.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 | train_loss : 0.7632 | train_acc : 0.6756 | valid loss : 0.5498 | valid_acc : 0.7800\n",
      "save the current best model | loss : 0.5498 | valid acc : 0.7800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:10<00:00, 40.95it/s]\n",
      "  1%|          | 4/450 [00:00<00:11, 38.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 | train_loss : 0.6883 | train_acc : 0.6578 | valid loss : 0.7149 | valid_acc : 0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:11<00:00, 40.04it/s]\n",
      "  1%|          | 4/450 [00:00<00:11, 38.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 | train_loss : 0.6488 | train_acc : 0.7067 | valid loss : 0.5866 | valid_acc : 0.7800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:12<00:00, 36.15it/s]\n",
      "  0%|          | 0/450 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 | train_loss : 0.6158 | train_acc : 0.7178 | valid loss : 0.6043 | valid_acc : 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:11<00:00, 38.25it/s]\n",
      "  1%|          | 4/450 [00:00<00:11, 38.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | train_loss : 0.6035 | train_acc : 0.7867 | valid loss : 0.5986 | valid_acc : 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:11<00:00, 40.65it/s]\n",
      "  1%|          | 4/450 [00:00<00:11, 39.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 | train_loss : 0.5862 | train_acc : 0.7533 | valid loss : 0.5437 | valid_acc : 0.7800\n",
      "save the current best model | loss : 0.5437 | valid acc : 0.7800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:12<00:00, 37.20it/s]\n",
      "  1%|          | 3/450 [00:00<00:17, 26.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | train_loss : 0.5402 | train_acc : 0.7800 | valid loss : 0.5098 | valid_acc : 0.8000\n",
      "save the current best model | loss : 0.5098 | valid acc : 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:15<00:00, 28.53it/s]\n",
      "  1%|          | 3/450 [00:00<00:16, 26.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 | train_loss : 0.5626 | train_acc : 0.7533 | valid loss : 0.6661 | valid_acc : 0.7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:12<00:00, 35.48it/s]\n",
      "  1%|          | 4/450 [00:00<00:12, 37.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | train_loss : 0.5570 | train_acc : 0.7622 | valid loss : 0.5622 | valid_acc : 0.7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:12<00:00, 37.05it/s]\n",
      "  1%|          | 4/450 [00:00<00:12, 35.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 | train_loss : 0.5052 | train_acc : 0.7556 | valid loss : 0.5474 | valid_acc : 0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:13<00:00, 33.11it/s]\n",
      "  1%|          | 4/450 [00:00<00:14, 30.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 | train_loss : 0.5307 | train_acc : 0.7822 | valid loss : 0.5585 | valid_acc : 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:14<00:00, 31.99it/s]\n",
      "  1%|          | 3/450 [00:00<00:15, 29.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 | train_loss : 0.4792 | train_acc : 0.7911 | valid loss : 0.5080 | valid_acc : 0.8000\n",
      "save the current best model | loss : 0.5080 | valid acc : 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:13<00:00, 31.53it/s]\n",
      "  1%|          | 4/450 [00:00<00:13, 33.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 | train_loss : 0.5533 | train_acc : 0.8178 | valid loss : 0.5041 | valid_acc : 0.8400\n",
      "save the current best model | loss : 0.5041 | valid acc : 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:14<00:00, 31.08it/s]\n",
      "  1%|          | 3/450 [00:00<00:15, 29.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 | train_loss : 0.5017 | train_acc : 0.8067 | valid loss : 0.5345 | valid_acc : 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:15<00:00, 27.80it/s]\n",
      "  1%|          | 3/450 [00:00<00:15, 29.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 | train_loss : 0.4819 | train_acc : 0.7933 | valid loss : 0.5595 | valid_acc : 0.7800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:16<00:00, 27.77it/s]\n",
      "  1%|          | 3/450 [00:00<00:21, 21.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 | train_loss : 0.4974 | train_acc : 0.7822 | valid loss : 0.5485 | valid_acc : 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:17<00:00, 24.89it/s]\n",
      "  1%|          | 3/450 [00:00<00:17, 26.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 | train_loss : 0.4835 | train_acc : 0.8111 | valid loss : 0.5208 | valid_acc : 0.7800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:19<00:00, 22.52it/s]\n",
      "  1%|          | 3/450 [00:00<00:18, 23.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 | train_loss : 0.4421 | train_acc : 0.8444 | valid loss : 0.5218 | valid_acc : 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:19<00:00, 24.17it/s]\n",
      "  1%|          | 3/450 [00:00<00:19, 23.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 | train_loss : 0.4642 | train_acc : 0.7933 | valid loss : 0.5411 | valid_acc : 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:19<00:00, 22.96it/s]\n",
      "  1%|          | 3/450 [00:00<00:20, 22.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 | train_loss : 0.4857 | train_acc : 0.8000 | valid loss : 0.5179 | valid_acc : 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:20<00:00, 21.58it/s]\n",
      "  0%|          | 2/450 [00:00<00:22, 19.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26 | train_loss : 0.4559 | train_acc : 0.8111 | valid loss : 0.5096 | valid_acc : 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:20<00:00, 21.46it/s]\n",
      "  0%|          | 2/450 [00:00<00:22, 19.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27 | train_loss : 0.4671 | train_acc : 0.8333 | valid loss : 0.5245 | valid_acc : 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:21<00:00, 20.15it/s]\n",
      "  1%|          | 3/450 [00:00<00:21, 20.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28 | train_loss : 0.4572 | train_acc : 0.8067 | valid loss : 0.5326 | valid_acc : 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:22<00:00, 20.67it/s]\n",
      "  0%|          | 2/450 [00:00<00:23, 19.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29 | train_loss : 0.4510 | train_acc : 0.8422 | valid loss : 0.5398 | valid_acc : 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 61/450 [00:03<00:20, 19.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-22083d9c339a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtrain_lsum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_lossAndAcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = Net(X_train.shape[2],256,300)\n",
    "# 定义优化器和损失函数 \n",
    "optimizer = Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "best_loss = 100\n",
    "for t in range(100):\n",
    "    train_lsum, n , acc = 0.0, 0, 0.0\n",
    "    count = 0\n",
    "    for each in tqdm(X_train):\n",
    "        d1 = each[0]\n",
    "        d2 = each[1]\n",
    "        d3 = each[2]\n",
    "        d1 = torch.tensor(d1).view(1,-1).float()\n",
    "        d2 = torch.tensor(d2).view(1,-1).float()\n",
    "        d3 = torch.tensor(d3).view(1,-1).float()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        d1_encoding = net.doc_encoding(d1)\n",
    "        d2_encoding = net.doc_encoding(d2)\n",
    "        d3_encoding = net.doc_encoding(d3)\n",
    "\n",
    "\n",
    "        d1_2 = F.cosine_similarity(d1_encoding,d2_encoding)\n",
    "\n",
    "        d1_3 = F.cosine_similarity(d1_encoding,d3_encoding)\n",
    "\n",
    "        if d1_2 > d1_3:\n",
    "            count+=1\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = net(d1,d2,d3)\n",
    "        train_lsum += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        n+=1\n",
    "    loss,acc = evaluate_lossAndAcc(net)\n",
    "    print(\"epoch %d | train_loss : %.4f | train_acc : %.4f | valid loss : %.4f | valid_acc : %.4f\" \n",
    "                       % (t, train_lsum/n,count / X_train.shape[0],loss, acc))\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        print('save the current best model | loss : %.4f | valid acc : %.4f' % (best_loss.item(), acc))\n",
    "        save_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 439.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count= 0 \n",
    "net.eval()\n",
    "for each in tqdm(X_valid):\n",
    "    d1 = each[0]\n",
    "    d2 = each[1]\n",
    "    d3 = each[2]\n",
    "    \n",
    "    d1 = torch.tensor(d1).view(1,-1).float()\n",
    "    d2 = torch.tensor(d2).view(1,-1).float()\n",
    "    d3 = torch.tensor(d3).view(1,-1).float()\n",
    "\n",
    "\n",
    "    d1_encoding = net.doc_encoding(d1)\n",
    "    d2_encoding = net.doc_encoding(d2)\n",
    "    d3_encoding = net.doc_encoding(d3)\n",
    "\n",
    "\n",
    "    d1_2 = F.cosine_similarity(d1_encoding,d2_encoding)\n",
    "\n",
    "    d1_3 = F.cosine_similarity(d1_encoding,d3_encoding)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if d1_2 > d1_3:\n",
    "        count+=1\n",
    "print('Acc :', count/X_valid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 717.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train : 50\n",
      "B acc : 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_dict = torch.load(os.path.join(MODEL_PATH, MODEL_NAME))\n",
    "net = Net(X_train.shape[2],256,300)\n",
    "\n",
    "net.load_state_dict(model_dict)\n",
    "net.eval()\n",
    "\n",
    "\n",
    "X_all = np.vstack((X_train, X_valid))\n",
    "\n",
    "\n",
    "test_y = []\n",
    "\n",
    "count= 0 \n",
    "for each in tqdm(X_valid):\n",
    "    d1 = each[0]\n",
    "    d2 = each[1]\n",
    "    d3 = each[2]\n",
    "    \n",
    "    d1 = torch.tensor(d1).view(1,-1).float()\n",
    "    d2 = torch.tensor(d2).view(1,-1).float()\n",
    "    d3 = torch.tensor(d3).view(1,-1).float()\n",
    "\n",
    "\n",
    "    d1_encoding = net.doc_encoding(d1)\n",
    "    d2_encoding = net.doc_encoding(d2)\n",
    "    d3_encoding = net.doc_encoding(d3)\n",
    "\n",
    "    d1_2 = F.cosine_similarity(d1_encoding,d2_encoding)\n",
    "\n",
    "    d1_3 = F.cosine_similarity(d1_encoding,d3_encoding)\n",
    "    \n",
    "    if d1_2 > d1_3:\n",
    "        count+=1\n",
    "        \n",
    "print('X_train :', X_valid.shape[0])\n",
    "print('B acc :', count/X_valid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

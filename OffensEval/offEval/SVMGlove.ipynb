{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import os\n",
    "from config import *\n",
    "from data_loader import *\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets as nlp_dset\n",
    "import nltk\n",
    "from torchtext.vocab import Vectors\n",
    "import random\n",
    "from torch.nn import init\n",
    "import torch.nn as nn\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2000\n",
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shawngung/Desktop/offEval/data_loader.py:12: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  self.dataframe = pd.DataFrame.from_csv(file, sep='\\t', header=0)\n",
      "[nltk_data] Downloading package punkt to /Users/shawngung/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shawngung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "dl = Data_Loader()\n",
    "train_df = dl.get_data()\n",
    "# dl.save_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfrom_for_scikit(task_header, text_field, label_field, embedding, train):\n",
    "    \"\"\"\n",
    "    task_header is one of subtask_a, subtask_b, subtask_c\n",
    "    \"\"\"\n",
    "    tokenised_train = [example.cleaned_s for example in train]\n",
    "    labels = np.array(\n",
    "      label_field.process(\n",
    "          [getattr(example, task_header) for example in train]\n",
    "      )\n",
    "    )\n",
    "\n",
    "    word_idxs = text_field.process(tokenised_train)\n",
    "    embeddings = torch.mean(embedding(word_idxs).detach(), dim=1)\n",
    "    return embeddings.numpy(), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 11916\n",
      "Validation size: 1324\n"
     ]
    }
   ],
   "source": [
    "#Create fields\n",
    "BATCH_SIZE = 128\n",
    "fix_length = 48\n",
    "TEXT = data.Field(\n",
    "    sequential=True, use_vocab=True, lower=True,\n",
    "    tokenize=nltk.word_tokenize, batch_first=True,\n",
    "    is_target=False, fix_length=fix_length)\n",
    "\n",
    "LABEL = data.LabelField(sequential=False, use_vocab=True, batch_first = True,is_target=True)\n",
    "ID = data.LabelField(sequential=False, use_vocab=False, batch_first=True)\n",
    "\n",
    "data_fields = {\n",
    "                \"cleaned_s\": ('cleaned_s', TEXT),\n",
    "                'subtask_a': ('subtask_a',LABEL),\n",
    "                'subtask_b': ('subtask_b',LABEL),\n",
    "                'subtask_c': ('subtask_c',LABEL),\n",
    "              }\n",
    "\n",
    "\n",
    "train = data.TabularDataset(os.path.join(DATA_DIR,PROCESSED_DATA_FILE), format='csv', fields = \n",
    "                            data_fields)\n",
    "\n",
    "train, valid = train.split(split_ratio=0.9, random_state=random.seed(SEED))\n",
    "\n",
    "print(f'Train size: {len(train)}')\n",
    "print(f'Validation size: {len(valid)}')\n",
    "\n",
    "vectors = Vectors(name='glove.6B.200d.txt', cache=GLOVE_DIR)\n",
    "#Now build vocab (using only the training set)\n",
    "TEXT.build_vocab(train, vectors=vectors) #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
    "\n",
    "\n",
    "LABEL.build_vocab(train.subtask_a)\n",
    "\n",
    "output_dim = len(LABEL.vocab)\n",
    "\n",
    "#Create iterators\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
    "                        batch_sizes=(BATCH_SIZE, len(valid)),  \n",
    "                        sort_key=lambda x: len(x.cleaned_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first tweet : ['thats', 'call', 'gun', 'control']\n",
      "first label : NOT\n"
     ]
    }
   ],
   "source": [
    "print('first tweet :', train[100].cleaned_s)\n",
    "print('first label :', train[100].subtask_a)\n",
    "# print(TEXT.vocab.stoi) # word to index\n",
    "# print(LABEL.vocab.stoi) # label to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(TEXT.vocab), embedding_dim)\n",
    "embedding.weight.data.copy_(TEXT.vocab.vectors) # copies pre-trained word vectors\n",
    "\n",
    "training_embeddings, training_labels = transfrom_for_scikit('subtask_a', TEXT, LABEL, embedding, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, average=False, class_weight={1: 2},\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=100,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l1',\n",
       "       power_t=0.5, random_state=42, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='hinge', penalty='l1',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=100, tol=None, class_weight={1: 2})\n",
    "clf.fit(training_embeddings, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embeddings, val_labels = transfrom_for_scikit('subtask_a', TEXT, LABEL, embedding, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[753 152]\n",
      " [178 241]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "NOT OFFENSIVE       0.81      0.83      0.82       905\n",
      "    OFFENSIVE       0.61      0.58      0.59       419\n",
      "\n",
      "    micro avg       0.75      0.75      0.75      1324\n",
      "    macro avg       0.71      0.70      0.71      1324\n",
      " weighted avg       0.75      0.75      0.75      1324\n",
      "\n",
      "Accuracy: 0.7507552870090635\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(val_embeddings)\n",
    "target_names = ['NOT OFFENSIVE','OFFENSIVE']\n",
    "print(metrics.confusion_matrix(val_labels, preds))\n",
    "print(metrics.classification_report(val_labels, preds,target_names = target_names))\n",
    "print(\"Accuracy:\", metrics.accuracy_score(val_labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3520\n",
      "Validation size: 880\n",
      "defaultdict(<function _default_unk_index at 0x1a224f2048>, {'TIN': 0, 'UNT': 1})\n"
     ]
    }
   ],
   "source": [
    "#Create fields\n",
    "BATCH_SIZE = 128\n",
    "fix_length = 48\n",
    "TEXT = data.Field(\n",
    "    sequential=True, use_vocab=True, lower=True,\n",
    "    tokenize=nltk.word_tokenize, batch_first=True,\n",
    "    is_target=False, fix_length=fix_length)\n",
    "\n",
    "LABEL = data.LabelField(sequential=False, use_vocab=True, batch_first = True,is_target=True)\n",
    "\n",
    "data_fields = {\n",
    "                \"cleaned_s\": ('cleaned_s', TEXT),\n",
    "                'subtask_a': ('subtask_a',LABEL),\n",
    "                'subtask_b': ('subtask_b',LABEL),\n",
    "              }\n",
    "\n",
    "\n",
    "train = data.TabularDataset(os.path.join(DATA_DIR,PROCESSED_DATA_FILE),\n",
    "                            format='csv',\n",
    "                            fields=data_fields,\n",
    "                            filter_pred=lambda d: d.subtask_a == 'OFF')\n",
    "\n",
    "train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
    "\n",
    "print(f'Train size: {len(train)}')\n",
    "print(f'Validation size: {len(valid)}')\n",
    "\n",
    "#Now build vocab (using only the training set)\n",
    "# This is where tokenization is performed on train\n",
    "\n",
    "vectors = Vectors(name='glove.6B.200d.txt', cache=GLOVE_DIR)\n",
    "#Now build vocab (using only the training set)\n",
    "TEXT.build_vocab(train, vectors=vectors) #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
    "LABEL.build_vocab(train.subtask_b)\n",
    "\n",
    "output_dim = len(LABEL.vocab)\n",
    "\n",
    "print(LABEL.vocab.stoi)\n",
    "\n",
    "#Create iterators\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
    "                        batch_sizes=(BATCH_SIZE, len(valid)),  \n",
    "                        sort_key=lambda x: len(x.tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "embedding = nn.Embedding(len(TEXT.vocab), embedding_dim)\n",
    "embedding.weight.data.copy_(TEXT.vocab.vectors) # copies pre-trained word vectors\n",
    "\n",
    "training_embeddings, training_labels = transfrom_for_scikit('subtask_b', TEXT, LABEL, embedding, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, average=False, class_weight={1: 6.8},\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=100,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l1',\n",
       "       power_t=0.5, random_state=42, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='hinge', penalty='l1',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=100, tol=None, class_weight={1: 6.8})\n",
    "clf.fit(training_embeddings, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embeddings, val_labels = transfrom_for_scikit('subtask_b', TEXT, LABEL, embedding, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[396 389]\n",
      " [ 23  72]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      TARGET       0.95      0.50      0.66       785\n",
      "    UNTARGET       0.16      0.76      0.26        95\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       880\n",
      "   macro avg       0.55      0.63      0.46       880\n",
      "weighted avg       0.86      0.53      0.61       880\n",
      "\n",
      "Accuracy: 0.5318181818181819\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(val_embeddings)\n",
    "target_names = ['TARGET','UNTARGET']\n",
    "print(metrics.confusion_matrix(val_labels, preds))\n",
    "print(metrics.classification_report(val_labels, preds,target_names=target_names))\n",
    "print(\"Accuracy:\", metrics.accuracy_score(val_labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3101\n",
      "Validation size: 775\n",
      "defaultdict(<function _default_unk_index at 0x1a224f2048>, {'IND': 0, 'GRP': 1, 'OTH': 2})\n"
     ]
    }
   ],
   "source": [
    "#Create fields\n",
    "BATCH_SIZE = 128\n",
    "TEXT = data.Field(\n",
    "    sequential=True, use_vocab=True, lower=True,\n",
    "    tokenize=nltk.word_tokenize, batch_first=True,\n",
    "    is_target=False)\n",
    "\n",
    "LABEL = data.LabelField(sequential=False, use_vocab=True, batch_first = True,is_target=True)\n",
    "\n",
    "\n",
    "data_fields = {\n",
    "                \"cleaned_s\": ('cleaned_s', TEXT),\n",
    "                'subtask_a': ('subtask_a',LABEL),\n",
    "                'subtask_b': ('subtask_b',LABEL),\n",
    "                'subtask_c': ('subtask_c',LABEL)\n",
    "              }\n",
    "\n",
    "train = data.TabularDataset(os.path.join(DATA_DIR,PROCESSED_DATA_FILE),\n",
    "                            format='csv',\n",
    "                            fields=data_fields,\n",
    "                            filter_pred=lambda d: d.subtask_a == 'OFF' and d.subtask_b == 'TIN')\n",
    "\n",
    "train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
    "\n",
    "print(f'Train size: {len(train)}')\n",
    "print(f'Validation size: {len(valid)}')\n",
    "\n",
    "#Now build vocab (using only the training set)\n",
    "\n",
    "vectors = Vectors(name='glove.6B.200d.txt', cache=GLOVE_DIR)\n",
    "#Now build vocab (using only the training set)\n",
    "TEXT.build_vocab(train, vectors=vectors) #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
    "LABEL.build_vocab(train.subtask_c)\n",
    "\n",
    "output_dim = len(LABEL.vocab)\n",
    "\n",
    "print(LABEL.vocab.stoi)\n",
    "\n",
    "#Create iterators\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
    "                        batch_sizes=(BATCH_SIZE, len(valid)),  \n",
    "                        sort_key=lambda x: len(x.tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(TEXT.vocab), embedding_dim)\n",
    "embedding.weight.data.copy_(TEXT.vocab.vectors) # copies pre-trained word vectors\n",
    "\n",
    "embeddings, training_labels = transfrom_for_scikit('subtask_c', TEXT, LABEL, embedding, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embeddings, val_labels = transfrom_for_scikit('subtask_c', TEXT, LABEL, embedding, valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, average=False,\n",
       "       class_weight={0: 1.6, 1: 3.7, 2: 8.4}, early_stopping=False,\n",
       "       epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=100, n_iter=None,\n",
       "       n_iter_no_change=5, n_jobs=None, penalty='l1', power_t=0.5,\n",
       "       random_state=42, shuffle=True, tol=None, validation_fraction=0.1,\n",
       "       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss='hinge', penalty='l1',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=100, tol=None, class_weight={0:1.6, 1:3.7, 2:8.4})\n",
    "\n",
    "clf.fit(embeddings, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[382  55  11]\n",
      " [105 104  26]\n",
      " [ 50  27  15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  INDIVIDUAL       0.71      0.85      0.78       448\n",
      "       GROUP       0.56      0.44      0.49       235\n",
      "       OTHER       0.29      0.16      0.21        92\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       775\n",
      "   macro avg       0.52      0.49      0.49       775\n",
      "weighted avg       0.62      0.65      0.62       775\n",
      "\n",
      "Accuracy: 0.6464516129032258\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(val_embeddings)\n",
    "target_names = ['INDIVIDUAL','GROUP','OTHER']\n",
    "\n",
    "print(metrics.confusion_matrix(val_labels, preds))\n",
    "print(metrics.classification_report(val_labels, preds,target_names= target_names))\n",
    "print(\"Accuracy:\", metrics.accuracy_score(val_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
